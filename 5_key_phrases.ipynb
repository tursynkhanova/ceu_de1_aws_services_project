{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee57b143",
   "metadata": {},
   "source": [
    "### Step 5: Now we will extract and compare key words across articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78d2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90a68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_REGION = 'eu-west-1' \n",
    "S3_BUCKET_NAME = 'aruzhan-sabira-hw3' \n",
    "\n",
    "# S3 prefixes containing ALL English-language articles (same as before)\n",
    "S3_INPUT_PREFIXES = [\n",
    "    'raw_articles/english/', \n",
    "    'translated_articles/'\n",
    "]\n",
    "\n",
    "# S3 output folder where the FINAL AGGREGATED TABLE will be saved\n",
    "S3_OUTPUT_TABLE = 'final_analysis_table/keywords_sentiment.csv'\n",
    "\n",
    "MAX_TEXT_BYTES = 4900 \n",
    "NOISE_PATTERNS = [ # To get more meaningful key phrases\n",
    "    # Navigation/Section Headings (New & Expanded)\n",
    "    'news', 'insights', 'events', 'platform', 'all stories', 'nation', 'astana', \n",
    "    'culture', 'sports', 'people', 'kazakhstan regions', 'state', 'vestbee',\n",
    "    'archive', 'items', 'homepage', 'mundo', 'loading', 'Mike Blake', 'Dana omirgazy',\n",
    "    \n",
    "    # Generic Junk & Symbols\n",
    "    '[[', ']]', '.txt', '.json', 'photo', 'picture', 'logo', 'copyright', 'shutterstock', \n",
    "    'illustration', 'view image', 'by author', 'twitter', 'facebook', 'instagram', \n",
    "    'linkedin', 'telegram', 'bookmark', 'pdf', 'oct', 'nov', 'dec', 'min read',\n",
    "    'language code', 'read time', 'share this', 'email', 'menu',\n",
    "    \n",
    "    # Specific Media Site Junk (Based on latest output)\n",
    "    'the asahi', 'the nikkei', 'the yomiuri shimbun', 'vc fund managers', \n",
    "    'vc summit', 'katarzyna groszkowska', 'lisa palchynska', 'kazakh', 'uzbekistan',\n",
    "    'new delhi', '5th world nomad games', 'tourism'\n",
    "    ]    \n",
    "\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "comprehend_client = boto3.client('comprehend', region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e84f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY_MAPPING = {\n",
    "    'ai-bubble-us-economy': 'The Guardian (USA)',\n",
    "    'are-we-in-an-ai-bubble-we-asked-european-investors': 'Poland (Vestbee)',\n",
    "    'japan-media-ai-threat': 'Japan (Japan Times)',\n",
    "    'kazakhstan-advances-ai-digital-ecosystem-developme': 'Kazakhstan (Astana Times)',\n",
    "    'uzbekistan-to-lay-off-over-2000-government-employe': 'Uzbekistan (Qaz Inform)',\n",
    "    'eksperty-vse-chasche-nazyvayut-bum-iskusstvennogo-': 'Russia (Meduza)',\n",
    "    'c11d1f63-a085-419d-bc62-030911459304': 'Australia (9 News)', \n",
    "    '124362358.cms': 'India (The Times of India)', \n",
    "    'blasen-bei-ki-werten-platzen-sie-bald-oder-geht-da': 'Germany (Handelsblatt)', \n",
    "    'resultados-da-oracle-sinalizam-bolha-de-ia-entenda': 'Brazil (CNN Brazil)',\n",
    "    'une-bulle-de-l-intelligence-artificielle': 'France (La Gazette)',\n",
    "    '1206507': 'Thailand (Bangkok BizNews)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f140cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_phrases(text_content):\n",
    "    # 1. Prepare byte-safe chunk for analysis\n",
    "    original_bytes = text_content.encode('utf-8')\n",
    "    total_bytes = len(original_bytes)\n",
    "    \n",
    "    if total_bytes == 0:\n",
    "        return []\n",
    "\n",
    "    # Get the first chunk (up to 4900 bytes)\n",
    "    end_index = min(MAX_TEXT_BYTES, total_bytes)\n",
    "    while end_index < total_bytes and (original_bytes[end_index] & 0xC0) == 0x80:\n",
    "        end_index -= 1\n",
    "\n",
    "    byte_chunk = original_bytes[:end_index]\n",
    "    \n",
    "    try:\n",
    "        string_chunk = byte_chunk.decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"  [ERROR] Unicode decode error on chunk. Cannot analyze.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        response = comprehend_client.detect_key_phrases(\n",
    "            Text=string_chunk, \n",
    "            LanguageCode='en'\n",
    "        )\n",
    "        \n",
    "        # 2. Filter key phrases based on confidence and noise patterns\n",
    "        key_phrases = []\n",
    "        for phrase in response['KeyPhrases']:\n",
    "            phrase_text = phrase['Text'].strip()\n",
    "            score = phrase['Score']\n",
    "            \n",
    "            # Filter 1: Confidence Score (Must be high for clean results)\n",
    "            if score < 0.90: \n",
    "                continue \n",
    "            \n",
    "            # Filter 2: Simple noise patterns and length check\n",
    "            is_noise = any(noise in phrase_text.lower() for noise in NOISE_PATTERNS)\n",
    "            is_too_short = len(phrase_text.split()) < 2\n",
    "            \n",
    "            if not is_noise and not is_too_short:\n",
    "                key_phrases.append((phrase_text, score))\n",
    "                \n",
    "        return key_phrases\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] Comprehend key phrase extraction failed. Error: {e}\")\n",
    "        return [(\"EXTRACTION_ERROR\", 0.0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73047d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_s3_file_for_keywords(bucket_name, input_key):\n",
    "    # 1. Download file content from S3\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=input_key)\n",
    "        original_text = response['Body'].read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] Failed to read object {input_key}. Skipping. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Determine the safe chunk for API calls (for sentiment)\n",
    "    text_to_analyze = original_text[:MAX_TEXT_BYTES] \n",
    "\n",
    "    # 2. Extract Key Phrases\n",
    "    key_phrase_data = extract_key_phrases(original_text)\n",
    "\n",
    "    # 3. Analyze Sentiment (for context)\n",
    "    try:\n",
    "        sentiment_response = comprehend_client.detect_sentiment(\n",
    "            Text=text_to_analyze, \n",
    "            LanguageCode='en'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        sentiment_response = {'Sentiment': 'ERROR', 'SentimentScore': {}}\n",
    "\n",
    "    # 4. Compile and return structured data\n",
    "    return {\n",
    "        'SourceFile': input_key,\n",
    "        'MediaSource': os.path.basename(input_key).replace('.txt', ''),\n",
    "        'Sentiment': sentiment_response['Sentiment'],\n",
    "        'KeyPhrases': [text for text, score in key_phrase_data],\n",
    "        'TopKeyPhrases': \", \".join([text for text, score in key_phrase_data][:5]) # Get top 5 as a string\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c32225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Full results saved to s3://aruzhan-sabira-hw3/final_analysis_table/keywords_sentiment.csv\n",
      "\n",
      "--- Final DataFrame Head (with Renamed MediaSource) ---\n",
      "| MediaSource               | Sentiment   | TopKeyPhrases                                                                                             |\n",
      "|:--------------------------|:------------|:----------------------------------------------------------------------------------------------------------|\n",
      "| The Guardian (USA)        | NEUTRAL     | The question, the AI bubble, the fallout, Eduardo Porter                                                  |\n",
      "|                           |             | Will, the bubble                                                                                          |\n",
      "| Poland (Vestbee)          | NEUTRAL     | the biggest, invitation-only event, an AI bubble, European investors, technology coverage, this year      |\n",
      "| Australia (9 News)        | NEUTRAL     | an AI tech bubble, the valuations, the company, a year, Artificial intelligence                           |\n",
      "| Japan (Japan Times)       | NEUTRAL     | a lawsuit, the Tokyo District Court, Generative artificial intelligence, a threat, journalistic integrity |\n",
      "| Kazakhstan (Astana Times) | NEUTRAL     | ’s Presidency, ’s Presidency, Digital Ecosystem Development, ’s Digital Headquarters, Dana Omirgazy       |\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    all_article_data = []\n",
    "\n",
    "    for prefix in S3_INPUT_PREFIXES:\n",
    "        file_list_response = s3_client.list_objects_v2(\n",
    "            Bucket=S3_BUCKET_NAME, \n",
    "            Prefix=prefix\n",
    "        )\n",
    "        \n",
    "        if 'Contents' not in file_list_response:\n",
    "            continue\n",
    "        \n",
    "        for obj in file_list_response['Contents']:\n",
    "            input_key = obj['Key']\n",
    "            \n",
    "            if input_key.endswith('/') or not input_key.endswith('.txt'):\n",
    "                continue\n",
    "            \n",
    "            # Process the file\n",
    "            # This calls the function that extracts keywords and sentiment\n",
    "            data = process_s3_file_for_keywords(S3_BUCKET_NAME, input_key)\n",
    "            if data:\n",
    "                all_article_data.append(data)\n",
    "            \n",
    "            time.sleep(1) \n",
    "\n",
    "    if not all_article_data:\n",
    "        print(\"\\nNo articles were successfully processed.\")\n",
    "    else:\n",
    "        final_df = pd.DataFrame(all_article_data)\n",
    "        \n",
    "        final_df['MediaSource_Clean'] = final_df['MediaSource'].apply(\n",
    "            lambda x: x.rsplit('_', 1)[0] if isinstance(x, str) and '_' in x else x\n",
    "        )\n",
    "\n",
    "        # Apply the mapping \n",
    "        final_df['Country/Region'] = final_df['MediaSource_Clean'].map(COUNTRY_MAPPING)\n",
    "        final_df['Country/Region'] = final_df['Country/Region'].fillna(final_df['MediaSource_Clean'])\n",
    "        final_df['MediaSource'] = final_df['Country/Region']\n",
    "        final_df = final_df.drop(columns=['MediaSource_Clean', 'Country/Region'])\n",
    "        \n",
    "        csv_buffer = io.StringIO()\n",
    "        final_df.to_csv(csv_buffer, index=False)\n",
    "        \n",
    "        s3_client.put_object(\n",
    "            Bucket=S3_BUCKET_NAME, \n",
    "            Key=S3_OUTPUT_TABLE, \n",
    "            Body=csv_buffer.getvalue().encode('utf-8')\n",
    "        )\n",
    "        print(f\"\\n✅ Full results saved to s3://{S3_BUCKET_NAME}/{S3_OUTPUT_TABLE}\")\n",
    "        \n",
    "        print(\"\\n--- Final DataFrame Head (with Renamed MediaSource) ---\")\n",
    "        print(final_df[['MediaSource', 'Sentiment', 'TopKeyPhrases']].head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36222c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MediaSource</th>\n",
       "      <th>TopKeyPhrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Guardian (USA)</td>\n",
       "      <td>The question, the AI bubble, the fallout, Edua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Poland (Vestbee)</td>\n",
       "      <td>the biggest, invitation-only event, an AI bubb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia (9 News)</td>\n",
       "      <td>an AI tech bubble, the valuations, the company...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japan (Japan Times)</td>\n",
       "      <td>a lawsuit, the Tokyo District Court, Generativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kazakhstan (Astana Times)</td>\n",
       "      <td>’s Presidency, ’s Presidency, Digital Ecosyste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Uzbekistan (Qaz Inform)</td>\n",
       "      <td>2,000 government employees, AI integration\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Thailand (Bangkok BizNews)</td>\n",
       "      <td>DOLLAR AI BUBBLE, BIG TECH FEARS MARCH, LOANS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>India (The Times of India)</td>\n",
       "      <td>The boom, Artificial Intelligence, the form, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany (Handelsblatt)</td>\n",
       "      <td>AI values, The numerous headlines warning, an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Russia (Meduza)</td>\n",
       "      <td>the AI boom, a financial bubble, tech companie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Brazil (CNN Brazil)</td>\n",
       "      <td>an AI bubble, the debate, the company, office ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>France (La Gazette)</td>\n",
       "      <td>An artificial intelligence bubble, The huge ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   MediaSource  \\\n",
       "0           The Guardian (USA)   \n",
       "1             Poland (Vestbee)   \n",
       "2           Australia (9 News)   \n",
       "3          Japan (Japan Times)   \n",
       "4    Kazakhstan (Astana Times)   \n",
       "5      Uzbekistan (Qaz Inform)   \n",
       "6   Thailand (Bangkok BizNews)   \n",
       "7   India (The Times of India)   \n",
       "8       Germany (Handelsblatt)   \n",
       "9              Russia (Meduza)   \n",
       "10         Brazil (CNN Brazil)   \n",
       "11         France (La Gazette)   \n",
       "\n",
       "                                        TopKeyPhrases  \n",
       "0   The question, the AI bubble, the fallout, Edua...  \n",
       "1   the biggest, invitation-only event, an AI bubb...  \n",
       "2   an AI tech bubble, the valuations, the company...  \n",
       "3   a lawsuit, the Tokyo District Court, Generativ...  \n",
       "4   ’s Presidency, ’s Presidency, Digital Ecosyste...  \n",
       "5   2,000 government employees, AI integration\\r\\n...  \n",
       "6   DOLLAR AI BUBBLE, BIG TECH FEARS MARCH, LOANS ...  \n",
       "7   The boom, Artificial Intelligence, the form, a...  \n",
       "8   AI values, The numerous headlines warning, an ...  \n",
       "9   the AI boom, a financial bubble, tech companie...  \n",
       "10  an AI bubble, the debate, the company, office ...  \n",
       "11  An artificial intelligence bubble, The huge ca...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_df = final_df[['MediaSource', 'TopKeyPhrases']]\n",
    "display_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
